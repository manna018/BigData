Script started on 2020-04-16 11:13:36+0530
##This file show find Customers based on Age Group
##Hive is running over yarn 
[01;32mmohit:[01;34m~/MyProjects/BigData/practice/hive[00m (master)[00m $ ssh m1
mohit@m1's password: 
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Sat Apr 11 17:44:47 2020 from 192.168.43.10
]0;mohit@m1: ~[01;32mmohit@m1[00m:[01;34m~[00m$ cd $HADOOP_INSTALL
]0;mohit@m1: /usr/local/hadoop[01;32mmohit@m1[00m:[01;34m/usr/local/hadoop[00m$ sbin/start-dfs.sh
Starting namenodes on [m1]
m1: starting namenode, logging to /usr/local/hadoop-2.6.5/logs/hadoop-mohit-namenode-m1.out
m3: starting datanode, logging to /usr/local/hadoop-2.6.5/logs/hadoop-mohit-datanode-m3.out
m2: starting datanode, logging to /usr/local/hadoop-2.6.5/logs/hadoop-mohit-datanode-m2.out
m1: starting datanode, logging to /usr/local/hadoop-2.6.5/logs/hadoop-mohit-datanode-m1.out
m3: WARNING: An illegal reflective access operation has occurred
m3: WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar) to method sun.security.krb5.Config.getInstance()
m3: WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
m3: WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
m3: WARNING: All illegal access operations will be denied in a future release
Starting secondary namenodes [m2]
m2: starting secondarynamenode, logging to /usr/local/hadoop-2.6.5/logs/hadoop-mohit-secondarynamenode-m2.out
]0;mohit@m1: /usr/local/hadoop[01;32mmohit@m1[00m:[01;34m/usr/local/hadoop[00m$ logout

Connection to m1 closed.
[01;32mmohit:[01;34m~/MyProjects/BigData/practice/hive[00m (master)[00m $ ssh m2
The authenticity of host 'm2 (192.168.43.139)' can't be established.
ECDSA key fingerprint is SHA256:4QS8suCLz8VUYQpDlAqZnASYClsrCqh3etO4PbVt2mc.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'm2,192.168.43.139' (ECDSA) to the list of known hosts.
mohit@m2's password: 
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Tue Apr  7 18:03:15 2020 from 192.168.43.10
]0;mohit@m2: ~[01;32mmohit@m2[00m:[01;34m~[00m$ cd $HADOOP_INSTALL
]0;mohit@m2: /usr/local/hadoop[01;32mmohit@m2[00m:[01;34m/usr/local/hadoop[00m$ sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop-2.6.5/logs/yarn-mohit-resourcemanager-m2.out
m1: starting nodemanager, logging to /usr/local/hadoop-2.6.5/logs/yarn-mohit-nodemanager-m1.out
m3: starting nodemanager, logging to /usr/local/hadoop-2.6.5/logs/yarn-mohit-nodemanager-m3.out
m2: starting nodemanager, logging to /usr/local/hadoop-2.6.5/logs/yarn-mohit-nodemanager-m2.out
m3: WARNING: An illegal reflective access operation has occurred
m3: WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar) to method sun.security.krb5.Config.getInstance()
m3: WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
m3: WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
m3: WARNING: All illegal access operations will be denied in a future release
]0;mohit@m2: /usr/local/hadoop[01;32mmohit@m2[00m:[01;34m/usr/local/hadoop[00m$ jps
4660 Jps
4101 SecondaryNameNode
4231 ResourceManager
3981 DataNode
4351 NodeManager
]0;mohit@m2: /usr/local/hadoop[01;32mmohit@m2[00m:[01;34m/usr/local/hadoop[00m$ logout
Connection to m2 closed.
[01;32mmohit:[01;34m~/MyProjects/BigData/practice/hive[00m (master)[00m $ ssh [K m3
The authenticity of host 'm3 (192.168.43.61)' can't be established.
ECDSA key fingerprint is SHA256:AB64gicV9wKNfWdc4HSsTNc2e9q64gAHNdEBOftTcxw.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'm3,192.168.43.61' (ECDSA) to the list of known hosts.
mohit@m3's password: 
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Fri Apr 10 16:18:59 2020 from 192.168.43.10
]0;mohit@m3: ~[01;32mmohit@m3[00m:[01;34m~[00m$ jps
3152 DataNode
3292 NodeManager
3471 Jps
]0;mohit@m3: ~[01;32mmohit@m3[00m:[01;34m~[00m$ logout
Connection to m3 closed.
[01;32mmohit:[01;34m~/MyProjects/BigData/practice/hive[00m (master)[00m $ ssh m1
mohit@m1's password: 
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Thu Apr 16 11:13:44 2020 from 192.168.43.115
]0;mohit@m1: ~[01;32mmohit@m1[00m:[01;34m~[00m$ jps
4576 Jps
4160 DataNode
4009 NameNode
4428 NodeManager
]0;mohit@m1: ~[01;32mmohit@m1[00m:[01;34m~[00m$ logout
Connection to m1 closed.
[01;32mmohit:[01;34m~/MyProjects/BigData/practice/hive[00m (master)[00m $ ssh m2
mohit@m2's password: 
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Thu Apr 16 11:14:33 2020 from 192.168.43.115
]0;mohit@m2: ~[01;32mmohit@m2[00m:[01;34m~[00m$ cd $AH[K[KHADOOP_INSTALL
]0;mohit@m2: /usr/local/hadoop[01;32mmohit@m2[00m:[01;34m/usr/local/hadoop[00m$ ls
[0m[01;34mbin[0m  [01;34minclude[0m  [01;34mlibexec[0m      [01;34mlogs[0m        README.txt  [01;34mshare[0m
[01;34metc[0m  [01;34mlib[0m      LICENSE.txt  NOTICE.txt  [01;34msbin[0m
]0;mohit@m2: /usr/local/hadoop[01;32mmohit@m2[00m:[01;34m/usr/local/hadoop[00m$ ls sbin
[0m[01;32mdistribute-exclude.sh[0m    [01;32mstart-all.cmd[0m        [01;32mstop-balancer.sh[0m
[01;32mhadoop-daemon.sh[0m         [01;32mstart-all.sh[0m         [01;32mstop-dfs.cmd[0m
[01;32mhadoop-daemons.sh[0m        [01;32mstart-balancer.sh[0m    [01;32mstop-dfs.sh[0m
[01;32mhdfs-config.cmd[0m          [01;32mstart-dfs.cmd[0m        [01;32mstop-secure-dns.sh[0m
[01;32mhdfs-config.sh[0m           [01;32mstart-dfs.sh[0m         [01;32mstop-yarn.cmd[0m
[01;32mhttpfs.sh[0m                [01;32mstart-secure-dns.sh[0m  [01;32mstop-yarn.sh[0m
[01;32mkms.sh[0m                   [01;32mstart-yarn.cmd[0m       [01;32myarn-daemon.sh[0m
[01;32mmr-jobhistory-daemon.sh[0m  [01;32mstart-yarn.sh[0m        [01;32myarn-daemons.sh[0m
[01;32mrefresh-namenodes.sh[0m     [01;32mstop-all.cmd[0m
[01;32mslaves.sh[0m                [01;32mstop-all.sh[0m
]0;mohit@m2: /usr/local/hadoop[01;32mmohit@m2[00m:[01;34m/usr/local/hadoop[00m$ logout
Connection to m2 closed.
[01;32mmohit:[01;34m~/MyProjects/BigData/practice/hive[00m (master)[00m $ ssh edh[Kge1
The authenticity of host 'edge1 (192.168.43.10)' can't be established.
ECDSA key fingerprint is SHA256:AB64gicV9wKNfWdc4HSsTNc2e9q64gAHNdEBOftTcxw.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'edge1' (ECDSA) to the list of known hosts.
mohit@edge1's password: 
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Tue Apr  7 18:04:36 2020 from 192.168.43.139
]0;mohit@edge1: ~[01;32mmohit@edge1[00m:[01;34m~[00m$ jps
3050 Jps
]0;mohit@edge1: ~[01;32mmohit@edge1[00m:[01;34m~[00m$ #lets look where the database is kept
]0;mohit@edge1: ~[01;32mmohit@edge1[00m:[01;34m~[00m$ #it must be in $HIVE_HOME beva[K[Kcause metastore_db is lying  there[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cmetastore_db is lying there[A]0;mohit@edge1: ~[01;32mmohit@edge1[00m:[01;34m~[00m$ [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cymetastore_db is lying there[A]0;mohit@edge1: ~[01;32mmohit@edge1[00m:[01;34m~[00m$ [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C metastore_db is lying there[A]0;mohit@edge1: ~[01;32mmohit@edge1[00m:[01;34m~[00m$ [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C metastore_db is lyi[1@n[A]0;mohit@edge1: ~[01;32mmohit@edge1[00m:[01;34m~[00m$ [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cmetastore_db is lyin[1P[A]0;mohit@edge1: ~[01;32mmohit@edge1[00m:[01;34m~[00m$ [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
[C[C[C[C[C[C[C[C
]0;mohit@edge1: ~[01;32mmohit@edge1[00m:[01;34m~[00m$ cd $HIVE_HOME
]0;mohit@edge1: /usr/local/hive[01;32mmohit@edge1[00m:[01;34m/usr/local/hive[00m$ ls
[0m[01;34massign4[0m    [01;34mexamples[0m  LICENSE         orderitem.java  RELEASE_NOTES.txt
[01;34mbin[0m        [01;34mhcatalog[0m  [01;34mmetastore_db[0m    output          [01;34mscripts[0m
[01;34mconf[0m       [01;34mjdbc[0m      NOTICE          [01;32mputInHive.sh[0m
derby.log  [01;34mlib[0m       orderitem.avsc  README.txt
]0;mohit@edge1: /usr/local/hive[01;32mmohit@edge1[00m:[01;34m/usr/local/hive[00m$ #yes its in assign [K4
]0;mohit@edge1: /usr/local/hive[01;32mmohit@edge1[00m:[01;34m/usr/local/hive[00m$ #lets start with hive
]0;mohit@edge1: /usr/local/hive[01;32mmohit@edge1[00m:[01;34m/usr/local/hive[00m$ #take a looo[Kk itnot [K[K[K[K[Knto assign4
]0;mohit@edge1: /usr/local/hive[01;32mmohit@edge1[00m:[01;34m/usr/local/hive[00m$ ls assign4
custs  practicals.txt  txns1.txt
]0;mohit@edge1: /usr/local/hive[01;32mmohit@edge1[00m:[01;34m/usr/local/hive[00m$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-2.1.0-bin/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/local/apache-hive-2.1.0-bin/lib/hive-common-2.1.0.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> so[8G[Khow databases;
OK
default
mannadb
retail
Time taken: 1.05 seconds, Fetched: 3 row(s)
hive> --we are using retail
hive> --means comment n[23G[Kin hive.[30G[K, multiline not supported
hive> use retail;
OK
Time taken: 0.027 seconds
hive> shoe[10G[Kw tables;
OK
txnrecords
txnrecsbycat
Time taken: 0.101 seconds, Fetched: 2 row(s)
hive> create table customer(custno string, firstname string, lastname s tring, age int,profession string)
    > row or[12G[K[11G[Kformat delimited
    > fields terminated by ''[29G[K,';
OK
Time taken: 1.124 seconds
hive> show tables;
OK
customer
txnrecords
txnrecsbycat
Time taken: 0.028 seconds, Fetched: 3 row(s)
hive> load data [16G[K[15G[K[14G[K[13G[K[12G[K[11G[K[10G[K[9G[K[8G[K[7G[K--locate custs in assign4 [32G[K. we have to put it in dt[56G[Katabase.
hive> load data [16G[K[15G[K[14G[K[13G[K[12G[K[11G[K[10G[K[9G[K[8G[K[7G[Kload data local inpath 'assign4/custs' into table custommer[65G[K[64G[K[63G[Ker;
Loading data to table retail.customer
OK
Time taken: 2.394 seconds
hive> --[8G[K[7G[Kse[8G[K[7G[Kdescc[11G[K customer;
OK
custno              	string              	                    
firstname           	string              	                    
lastname            	string              	                    
age                 	int                 	                    
profession          	string              	                    
Time taken: 0.103 seconds, Fetched: 5 row(s)
hive> select * from customer limit 5;
OK
4000001	Kristina	Chung	55	Pilot
4000002	Paige	Chen	74	Teacher
4000003	Sherri	Melton	34	Firefighter
4000004	Gretchen	Hill	66	Computer hardware engineer
4000005	Karen	Puckett	74	Lawyer
Time taken: 1.28 seconds, Fetched: 5 row(s)
hive> select count(cud[22G[Kstno) froom [33G[K[32G[K[31G[Km customer;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = mohit_20200416112812_c6b5addb-53fe-482c-a5c3-61b39f65a53f
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1587015889950_0001, Tracking URL = http://m2:8088/proxy/application_1587015889950_0001/
Kill Command = /usr/local/hadoop-2.6.5/bin/hadoop job  -kill job_1587015889950_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-04-16 11:28:25,577 Stage-1 map = 0%,  reduce = 0%
2020-04-16 11:29:26,068 Stage-1 map = 0%,  reduce = 0%
2020-04-16 11:30:26,852 Stage-1 map = 0%,  reduce = 0%
2020-04-16 11:31:27,436 Stage-1 map = 0%,  reduce = 0%
2020-04-16 11:31:34,727 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.48 sec
2020-04-16 11:31:43,118 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.3 sec
MapReduce Total cumulative CPU time: 3 seconds 300 msec
Ended Job = job_1587015889950_0001
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.3 sec   HDFS Read: 399557 HDFS Write: 104 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 300 msec
OK
9999
Time taken: 211.636 seconds, Fetched: 1 row(s)
hive> -- it started a map reduce task
hive> -- we coul [17G[Kd watch it on the link provided:
hive> -- http://stackoverflow.com/questions/37466913/how-toarting%20-ma ke-a-dynamic-limit-in-mysql[27G[K[26G[K[25G[K[24G[K[23G[K[22G[K[21G[K[20G[K[19G[K[18G[K[17G[K[16G[K[15G[K[14G[K[13G[K[12G[K[11G[K[10G[K[9G[K[8G[K[7G[K[6G[K[5G[K[4G[K[3G[K[2G[K[1G[K[1A[71G[K[K[70G[K[69G[K[68G[K[67G[K[66G[K[65G[K[64G[K[63G[K[62G[K[61G[K[60G[K[59G[K[58G[K[57G[K[56G[K[55G[K[54G[K[53G[K[52G[K[51G[K[50G[K[49G[K[48G[K[47G[K[46G[K[45G[K[44G[K[43G[K[42G[K[41G[K[40G[K[39G[K[38G[K[37G[K[36G[K[35G[K[34G[K[33G[K[32G[K[31G[K[30G[K[29G[K[28G[K[27G[K[26G[K[25G[K[24G[K[23G[K[22G[K[21G[K[20G[K[19G[K[18G[K[17G[K[16G[K[15G[K[14G[K[13G[K[12G[K[11G[K[10G[K[9G[K (Tracking URL)
hive> -- we missed to start job history server
hive> =[7G[K-- itt[12G[K can be done 
hive> --on mohit [17G[K [17G[K@m2:$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start his toryserver
hive> --escapig[15G[Kng it now
hive> crae[10G[K[9G[Keate table out1([24G[K(custno int,firstname string,age int,profession  string,amount double,product string)
    > row format delimited 
    > field [12G[Ks terminated by ',';
OK
Time taken: 0.153 seconds
hive> insert [13G[K[12G[K[11G[K[10G[K[9G[K[8G[K[7G[Kdesc oi[13G[Kut1;
OK
custno              	int                 	                    
firstname           	string              	                    
age                 	int                 	                    
profession          	string              	                    
amount              	double              	                    
product             	string              	                    
Time taken: 0.092 seconds, Fetched: 6 row(s)
hive> ---y[10G[Kits same as customer [30G[K[29G[K[28G[K[27G[K[26G[K[25G[K[24G[K[23G[K[22G[K[21G[K[20G[K[19G[K[18G[K[17G[K[16G[K[15G[K[14G[K[13G[K[12G[K[11G[K[10G[K[9G[K[8G[K[7G[Kdesc customer;
OK
custno              	string              	                    
firstname           	string              	                    
lastname            	string              	                    
age                 	int                 	                    
profession          	string              	                    
Time taken: 0.091 seconds, Fetched: 5 row(s)
hive> -- amount ans [20G[K[19G[Kd producr[27G[Kt io[30G[Kf[30G[Knfo  comes from t[46G[K[45G[K[44G[K[43G[K[42G[K[41G[K[40G[K[39G[K[38G[K[37G[K[36G[K[35G[K[34G[K[33G[K[32G[K[31G[K[30G[K[29G[K[28G[K[27G[K[26G[K[25G[K[24G[K[23G[K[22G[K[21G[K[20G[K[19G[K[18G[K[17G[K[16G[K[15G[K[14G[K[13G[K[12G[K[11G[K[10G[K[9G[K[8G[K[7G[Kdesc txnrecords;
OK
txnno               	int                 	                    
txndate             	string              	                    
custno              	int                 	                    
amount              	double              	                    
category            	string              	                    
product             	string              	                    
city                	string              	                    
state               	string              	                    
spendby             	string              	                    
Time taken: 0.218 seconds, Fetched: 9 row(s)
hive> -- product and amount goes into out1'
hive> insert overwrite table out1
    > select a.custno,a.firstname,a.age,a.professionb.[54G[K[53G[K,b.producr[62G[Kt [63G[K[62G[K[61G[K[60G[K[59G[K[58G[K[57G[K[56G[Kamount,b.product 
    > from customer a JOIN txnrecords b ON a.custno=b.custno;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = mohit_20200416114051_8b9cc171-41a7-4aea-a983-1001e6c2c22f
Total jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-2.1.0-bin/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2020-04-16 11:40:56	Starting to launch local task to process map join;	maximum memory = 518979584
2020-04-16 11:40:58	Dump the side-table for tag: 0 with group count: 9999 into file: file:/tmp/mohit/a993596a-a7ff-4821-8655-f3ea9e01cab4/hive_2020-04-16_11-40-51_478_8153478797335008366-1/-local-10002/HashTable-Stage-4/MapJoin-mapfile00--.hashtable
2020-04-16 11:40:58	Uploaded 1 File to: file:/tmp/mohit/a993596a-a7ff-4821-8655-f3ea9e01cab4/hive_2020-04-16_11-40-51_478_8153478797335008366-1/-local-10002/HashTable-Stage-4/MapJoin-mapfile00--.hashtable (553952 bytes)
2020-04-16 11:40:58	End of local task; Time Taken: 1.519 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1587015889950_0002, Tracking URL = http://m2:8088/proxy/application_1587015889950_0002/
Kill Command = /usr/local/hadoop-2.6.5/bin/hadoop job  -kill job_1587015889950_0002
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 0
2020-04-16 11:41:07,090 Stage-4 map = 0%,  reduce = 0%
2020-04-16 11:41:14,328 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 2.89 sec
MapReduce Total cumulative CPU time: 2 seconds 890 msec
Ended Job = job_1587015889950_0002
Loading data to table retail.out1
MapReduce Jobs Launched: 
Stage-Stage-4: Map: 1   Cumulative CPU: 2.89 sec   HDFS Read: 4426457 HDFS Write: 2530247 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 890 msec
OK
Time taken: 25.285 seconds
hive> select * from out1 limit 100;
OK
4007024	Cameron	59	Actor	40.33	Cardio Machine Accessories
4006742	Gregory	36	Accountant	198.44	Weightlifting Gloves
4009775	Ruby	44	Designer	5.58	Weightlifting Machine Accessories
4002199	Keith	44	Police officer	198.19	Gymnastics Rings
4002613	Hugh	43	Engineering technician	98.81	Field Hockey
4007591	Jennifer	54	Electrician	193.63	Camping & Backpacking & Hiking
4002190	Sheryl	62	Designer	27.89	Jigsaw Puzzles
4002964	Ken	67	Recreation and fitness worker	96.01	Sandboxes
4007361	Terri	52	Loan officer	10.44	Snowmobiling
4004798	Geoffrey	65	Chemist	152.46	Bungee Jumping
4004646	Frank	64	Computer software engineer	180.28	Archery
4008071	Ruby	46	Agricultural and food scientist	121.39	Swing Sets
4002473	Regina	68	Pilot	41.52	Bowling
4003268	Lillian	35	Firefighter	107.8	Field Hockey
4004613	Dennis	62	Judge	36.81	Vaulting Horses
4003179	Kelly	71	Engineering technician	137.64	Fencing
4009135	Elaine	42	Electrician	35.56	Free Weight Bars
4006679	Jeanne	22	Judge	75.55	Scuba Diving & Snorkeling
4002444	Jeanne	72	Therapist	88.65	Baseball
4008871	Nina	69	Politician	51.81	Life Jackets
4001364	Nancy	75	Judge	41.55	Weightlifting Belts
4003144	Samantha	65	Veterinarian	45.79	Parachutes
4006131	Ken	25	Lawyer	19.64	Kitesurfing
4007596	Sandra	30	Actor	99.5	Gymnastics Rings
4009341	Todd	31	Farmer	151.2	Surfing
4003760	Suzanne	25	Computer support specialist	144.2	Darts
4000403	Leo	65	Pilot	31.58	Wrestling
4005211	Jay	36	Photographer	66.4	Mahjong
4001864	Alexandra	29	Carpenter	79.78	Cricket
4005691	Brenda	54	Reporter	126.9	Hunting
4009693	Paul	43	Veterinarian	47.05	Swimming
4002130	Colleen	55	Civil engineer	5.03	Dice & Dice Sets
4007790	Clifford	24	Librarian	20.13	Soccer
4005337	Allison	73	Human resources assistant	154.15	Lawn Games
4000663	Claire	29	Police officer	98.96	Indoor Volleyball
4006967	Martin	54	Computer support specialist	185.26	Board Games
4009055	Eva	68	Veterinarian	35.66	Football
4005737	Dana	72	Artist	20.2	Shooting Games
4000175	Ben	35	Computer support specialist	150.6	Camping & Backpacking & Hiking
4001873	Phyllis	24	Farmer	174.36	Swing Sets
4006442	Jerome	29	Automotive mechanic	165.1	Cheerleading
4004237	Gregory	74	Actor	28.11	Bowling
4007470	Rick	56	Coach	38.52	Tetherball
4002554	Laura	61	Financial analyst	32.34	Water Polo
4001041	Crystal	39	Psychologist	135.37	Surfing
4005646	Tammy	65	Reporter	90.04	Abdominal Equipment
4005580	Eddie	72	Automotive mechanic	52.29	Vaulting Horses
4009698	Hilda	48	Firefighter	100.1	Swing Sets
4009252	Phillip	23	Firefighter	157.94	Exercise Bands
4003896	Nicole	74	Judge	144.59	Jumping Stilts
4005578	Hilda	64	Architect	55.93	Pogo Sticks
4002323	Roger	29	Recreation and fitness worker	32.65	Life Jackets
4008289	David	25	Electrical engineer	44.82	Lawn Water Slides
4003091	Joanne	59	Politician	44.46	Scuba Diving & Snorkeling
4007357	Clara	61	Computer software engineer	154.87	Running
4004961	Katharine	34	Computer hardware engineer	106.11	Swimming
4008744	Larry	47	Police officer	176.63	Geocaching
4007745	Sally	66	Dancer	178.2	Skating
4003248	Randall	33	Computer support specialist	194.86	Windsurfing
4002854	Leon	51	Pharmacist	21.43	Snowboarding
4004874	Danielle	48	Reporter	118.18	Cardio Machine Accessories
4009680	Ted	38	Electrician	41.14	Weightlifting Machine Accessories
4000539	Hugh	71	Firefighter	100.93	Beach Volleyball
4009719	Greg	71	Musician	129.26	Downhill Skiing
4008455	Laura	59	Computer hardware engineer	105.24	Weightlifting Machine Accessories
4005887	Neil	45	Physicist	66.06	Riding Scooters
4006293	Nicholas	63	Teacher	89.14	Tetherball
4002526	Bob	40		159.14	Poker Chips & Sets
4006736	Tamara	23	Loan officer	171.57	Ballet Bars
4005403	Ruby	48	Veterinarian	89.91	Softball
4006291	Ruby	57	Police officer	32.28	Skating
4009360	Raymond	39	Doctor	152.21	Cricket
4001305	Andrea	56	Psychologist	150.82	Skateboarding
4008653	Toni	39	Athlete	133.2	Riding Scooters
4002106	Lester	50	Coach	148.31	Portable Electronic Games
4006262	Denise	33	Financial analyst	125.28	Wrestling
4003860	Pamela	71	Veterinarian	170.05	Trampolines
4000458	Harold	40	Musician	74.06	Jigsaw Puzzles
4002200	Sandy	33	Financial analyst	175.24	Baseball
4005751	Paige	29	Nurse	39.8	Springboards
4005625	Herman	30	Architect	174.82	Ice Climbing
4003500	Dana	51	Artist	188.9	Swimming
4001098	Curtis	50	Loan officer	21.23	Gymnastics Rings
4007259	Aaron	38	Engineering technician	22.42	Rock Climbing
4000293	Kay	44	Childcare worker	49.97	Stopwatches
4008024	Derek	66	Automotive mechanic	31.84	Swimming
4008884	Warren	74	Politician	80.99	Tetherball
4001050	Barbara	73	Actor	89.56	Gymnastics Mats
4003309	Jacob	35	Reporter	55.35	Towed Water Sports
4001561	Arlene	51		184.56	Cheerleading
4009270	Christy	71	Loan officer	176.34	Parachutes
4002697	Joan	74	Librarian	35.75	Running
4008469	William	62	Designer	42.51	Team Handball
4006425	Joe	30	Economist	193.11	Sledding
4005513	Valerie	62	Dancer	68.86	Jumping Stilts
4004611	Wendy	27	Architect	146.36	Whitewater Rafting
4005227	Diane	57	Firefighter	130.52	Medicine Balls
4002299	Alison	56	Electrician	197.54	Exercise Balls
4002707	Dana	28	Loan officer	5.95	Dominoes
4006562	Valerie	44	Computer software engineer	37.29	Lawn Water Slides
Time taken: 0.211 seconds, Fetched: 100 row(s)
hive> create table out2(custno int,firstname string,age int,profession  string,amount double,product string, level string)
    > row format delimited
    > fields terminated by ',';
OK
Time taken: 0.217 seconds
hive> insert overr[18G[Kwrite ta[25G[K[24G[K[23G[K[22G[K[21G[K[20G[K[19G[K[18G[K[17G[K[16G[K[15G[K[14G[K[13G[K[12G[K[11G[K[10G[K[9G[K[8G[K[7G[Kdesc out2;
OK
custno              	int                 	                    
firstname           	string              	                    
age                 	int                 	                    
profession          	string              	                    
amount              	double              	                    
product             	string              	                    
level               	string              	                    
Time taken: 0.059 seconds, Fetched: 7 row(s)
hive> desc out1;
OK
custno              	int                 	                    
firstname           	string              	                    
age                 	int                 	                    
profession          	string              	                    
amount              	double              	                    
product             	string              	                    
Time taken: 0.06 seconds, Fetched: 6 row(s)
hive> -- out2 has extra r[25G[Kcolumn level we put a mapping of age there
hive> insert overwrie[21G[Kte table out2
    > select *,case
    > when age<30 then 'low'
    > eh[8G[K[7G[Kwhen age>=30 and age <50 [31G[K[30G[K[29G[K 50 then 'middle';[46G[K
    > when age>=50 then 'old'
    > else 'others'
    > end
    > from out1;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = mohit_20200416115035_36bbb2bb-03fd-4c91-a232-50138eb2e462
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1587015889950_0003, Tracking URL = http://m2:8088/proxy/application_1587015889950_0003/
Kill Command = /usr/local/hadoop-2.6.5/bin/hadoop job  -kill job_1587015889950_0003
Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 0
2020-04-16 11:56:37,743 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_1587015889950_0003 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive> ---------connection exception occuree[43G[K[42G[K[41G[Krred [45G[K[44G[K[43G[K[42G[K[41G[K[40G[K[39G[K[38G[K[37G[K[36G[K[35G[K[34G[K[33G[K[32G[K[31G[K[30G[K[29G[K[28G[K[27G[K[26G[K[25G[K[24G[K[23G[K[22G[K[21G[K[20G[K[19G[K[18G[K[17G[K[16G[K[15G[K[14G[K[13G[K[12G[K[11G[K[10G[K[9G[K[8G[K- connect exception occurred lets tr[43G[K[42G[Kretry
hive> -- connect exception occurred lets retry[7G[Kfrom out1;[7G[Kend[8G[Klse 'others'[7G[Kwhen age>=50 then 'old'[17G[K30 and age < 50 then 'middle'[15G[K<30 then 'low'[7G[Kselect *,case[7G[Kinsert overwrite table out2[7G[K-- out2 has extra column level we put a mapping of age there[7G[Kinsert overwrite table out2
    > insert overwrite table out2[7G[K-- connect exception occurred lets retry[7G[Kfrom out1;[7G[Kend[8G[Klse 'others'[7G[Kwhen age>=50 then 'old'[17G[K30 and age < 50 then 'middle'[15G[K<30 then 'low'[7G[Kselect *,case[7G[Kinsert overwrite table out2[7G[Kselect *,case
    > select *,case[7G[Kinsert overwrite table out2[7G[K-- connect exception occurred lets retry[7G[Kfrom out1;[7G[Kend[8G[Klse 'others'[7G[Kwhen age>=50 then 'old'[17G[K30 and age < 50 then 'middle'[17G[K50 then 'old'[17G[K30 and age < 50 then 'middle'[15G[K<30 then 'low'[7G[Kselect *,case[7G[Kwhen age<30 then 'low'
    > when age<30 then 'low'[7G[Kselect *,case[7G[Kinsert overwrite table out2[7G[K-- connect exception occurred lets retry[7G[Kfrom out1;[7G[Kend[8G[Klse 'others'[7G[Kwhen age>=50 then 'old'[17G[K30 and age < 50 then 'middle'[15G[K<30 then 'low'[7G[Kselect *,case[7G[Kwhen age<30 then 'low'[15G[K>=30 and age < 50 then 'middle'
    > when age>=30 and age < 50 then 'middle'[15G[K<30 then 'low'[7G[Kselect *,case[7G[Kinsert overwrite table out2[7G[K-- connect exception occurred lets retry[7G[Kfrom out1;[7G[Kend[8G[Klse 'others'[7G[Kwhen age>=50 then 'old'[17G[K30 and age < 50 then 'middle'[17G[K50 then 'old'
    > when age>=50 then 'old'[17G[K30 and age < 50 then 'middle'[15G[K<30 then 'low'[7G[Kselect *,case[7G[Kinsert overwrite table out2[7G[K-- connect exception occurred lets retry[7G[Kfrom out1;[7G[Kend[8G[Klse 'others'[7G[Kwhen age>=50 then 'old'[7G[Kelse 'others'
    > else 'others'[7G[Kwhen age>=50 then 'old'[17G[K30 and age < 50 then 'middle'[15G[K<30 then 'low'[7G[Kselect *,case[7G[Kinsert overwrite table out2[7G[K-- connect exception occurred lets retry[7G[Kfrom out1;[7G[K-- connect exception occurred lets retry[7G[Kinsert overwrite table out2[7G[Kselect *,case[7G[Kwhen age<30 then 'low'[15G[K>=30 and age < 50 then 'middle'[17G[K50 then 'old'[7G[Kelse 'others'[7G[Kelse 'others'[7G[Kwhen age>=50 then 'old'[17G[K30 and age < 50 then 'middle'[15G[K<30 then 'low'[7G[Kselect *,case[7G[Kinsert overwrite table out2[7G[K-- connect exception occurred lets retry[7G[Kfrom out1;[7G[Kend[8G[Klse 'others'[8G[Knd
    > end[8G[Klse 'others'[7G[Kwhen age>=50 then 'old'[17G[K30 and age < 50 then 'middle'[15G[K<30 then 'low'[7G[Kselect *,case[7G[Kinsert overwrite table out2[7G[K-- connect exception occurred lets retry[7G[Kfrom out1;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = mohit_20200416120649_a2fcb52b-ecf3-4ca4-aa3e-b91194a16f21
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1587015889950_0004, Tracking URL = http://m2:8088/proxy/application_1587015889950_0004/
Kill Command = /usr/local/hadoop-2.6.5/bin/hadoop job  -kill job_1587015889950_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2020-04-16 12:06:56,176 Stage-1 map = 0%,  reduce = 0%
2020-04-16 12:07:56,990 Stage-1 map = 0%,  reduce = 0%
2020-04-16 12:08:57,532 Stage-1 map = 0%,  reduce = 0%
2020-04-16 12:09:58,050 Stage-1 map = 0%,  reduce = 0%
2020-04-16 12:10:07,482 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.17 sec
MapReduce Total cumulative CPU time: 2 seconds 170 msec
Ended Job = job_1587015889950_0004
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://m1:9000/user/hive/warehouse/retail.db/out2/.hive-staging_hive_2020-04-16_12-06-49_558_5063276530985635205-1/-ext-10000
Loading data to table retail.out2
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 2.17 sec   HDFS Read: 2535512 HDFS Write: 2784776 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 170 msec
OK
Time taken: 199.287 seconds
hive> descriv[13G[Kbe [15G[K[14G[K[13G[K[12G[K[11G[K[10G[K[9G[K[8G[K[7G[Kelect [12G[K[11G[K[10G[K[9G[K[8G[K[7G[Kselect * from out2 limit 1000[35G[K;
OK
4007024	Cameron	59	Actor	40.33	Cardio Machine Accessories	old
4006742	Gregory	36	Accountant	198.44	Weightlifting Gloves	middle
4009775	Ruby	44	Designer	5.58	Weightlifting Machine Accessories	middle
4002199	Keith	44	Police officer	198.19	Gymnastics Rings	middle
4002613	Hugh	43	Engineering technician	98.81	Field Hockey	middle
4007591	Jennifer	54	Electrician	193.63	Camping & Backpacking & Hiking	old
4002190	Sheryl	62	Designer	27.89	Jigsaw Puzzles	old
4002964	Ken	67	Recreation and fitness worker	96.01	Sandboxes	old
4007361	Terri	52	Loan officer	10.44	Snowmobiling	old
4004798	Geoffrey	65	Chemist	152.46	Bungee Jumping	old
4004646	Frank	64	Computer software engineer	180.28	Archery	old
4008071	Ruby	46	Agricultural and food scientist	121.39	Swing Sets	middle
4002473	Regina	68	Pilot	41.52	Bowling	old
4003268	Lillian	35	Firefighter	107.8	Field Hockey	middle
4004613	Dennis	62	Judge	36.81	Vaulting Horses	old
4003179	Kelly	71	Engineering technician	137.64	Fencing	old
4009135	Elaine	42	Electrician	35.56	Free Weight Bars	middle
4006679	Jeanne	22	Judge	75.55	Scuba Diving & Snorkeling	low
4002444	Jeanne	72	Therapist	88.65	Baseball	old
4008871	Nina	69	Politician	51.81	Life Jackets	old
4001364	Nancy	75	Judge	41.55	Weightlifting Belts	old
4003144	Samantha	65	Veterinarian	45.79	Parachutes	old
4006131	Ken	25	Lawyer	19.64	Kitesurfing	low
4007596	Sandra	30	Actor	99.5	Gymnastics Rings	middle
4009341	Todd	31	Farmer	151.2	Surfing	middle
4003760	Suzanne	25	Computer support specialist	144.2	Darts	low
4000403	Leo	65	Pilot	31.58	Wrestling	old
4005211	Jay	36	Photographer	66.4	Mahjong	middle
4001864	Alexandra	29	Carpenter	79.78	Cricket	low
4005691	Brenda	54	Reporter	126.9	Hunting	old
4009693	Paul	43	Veterinarian	47.05	Swimming	middle
4002130	Colleen	55	Civil engineer	5.03	Dice & Dice Sets	old
4007790	Clifford	24	Librarian	20.13	Soccer	low
4005337	Allison	73	Human resources assistant	154.15	Lawn Games	old
4000663	Claire	29	Police officer	98.96	Indoor Volleyball	low
4006967	Martin	54	Computer support specialist	185.26	Board Games	old
4009055	Eva	68	Veterinarian	35.66	Football	old
4005737	Dana	72	Artist	20.2	Shooting Games	old
4000175	Ben	35	Computer support specialist	150.6	Camping & Backpacking & Hiking	middle
4001873	Phyllis	24	Farmer	174.36	Swing Sets	low
4006442	Jerome	29	Automotive mechanic	165.1	Cheerleading	low
4004237	Gregory	74	Actor	28.11	Bowling	old
4007470	Rick	56	Coach	38.52	Tetherball	old
4002554	Laura	61	Financial analyst	32.34	Water Polo	old
4001041	Crystal	39	Psychologist	135.37	Surfing	middle
4005646	Tammy	65	Reporter	90.04	Abdominal Equipment	old
4005580	Eddie	72	Automotive mechanic	52.29	Vaulting Horses	old
4009698	Hilda	48	Firefighter	100.1	Swing Sets	middle
4009252	Phillip	23	Firefighter	157.94	Exercise Bands	low
4003896	Nicole	74	Judge	144.59	Jumping Stilts	old
4005578	Hilda	64	Architect	55.93	Pogo Sticks	old
4002323	Roger	29	Recreation and fitness worker	32.65	Life Jackets	low
4008289	David	25	Electrical engineer	44.82	Lawn Water Slides	low
4003091	Joanne	59	Politician	44.46	Scuba Diving & Snorkeling	old
4007357	Clara	61	Computer software engineer	154.87	Running	old
4004961	Katharine	34	Computer hardware engineer	106.11	Swimming	middle
4008744	Larry	47	Police officer	176.63	Geocaching	middle
4007745	Sally	66	Dancer	178.2	Skating	old
4003248	Randall	33	Computer support specialist	194.86	Windsurfing	middle
4002854	Leon	51	Pharmacist	21.43	Snowboarding	old
4004874	Danielle	48	Reporter	118.18	Cardio Machine Accessories	middle
4009680	Ted	38	Electrician	41.14	Weightlifting Machine Accessories	middle
4000539	Hugh	71	Firefighter	100.93	Beach Volleyball	old
4009719	Greg	71	Musician	129.26	Downhill Skiing	old
4008455	Laura	59	Computer hardware engineer	105.24	Weightlifting Machine Accessories	old
4005887	Neil	45	Physicist	66.06	Riding Scooters	middle
4006293	Nicholas	63	Teacher	89.14	Tetherball	old
4002526	Bob	40		159.14	Poker Chips & Sets	middle
4006736	Tamara	23	Loan officer	171.57	Ballet Bars	low
4005403	Ruby	48	Veterinarian	89.91	Softball	middle
4006291	Ruby	57	Police officer	32.28	Skating	old
4009360	Raymond	39	Doctor	152.21	Cricket	middle
4001305	Andrea	56	Psychologist	150.82	Skateboarding	old
4008653	Toni	39	Athlete	133.2	Riding Scooters	middle
4002106	Lester	50	Coach	148.31	Portable Electronic Games	old
4006262	Denise	33	Financial analyst	125.28	Wrestling	middle
4003860	Pamela	71	Veterinarian	170.05	Trampolines	old
4000458	Harold	40	Musician	74.06	Jigsaw Puzzles	middle
4002200	Sandy	33	Financial analyst	175.24	Baseball	middle
4005751	Paige	29	Nurse	39.8	Springboards	low
4005625	Herman	30	Architect	174.82	Ice Climbing	middle
4003500	Dana	51	Artist	188.9	Swimming	old
4001098	Curtis	50	Loan officer	21.23	Gymnastics Rings	old
4007259	Aaron	38	Engineering technician	22.42	Rock Climbing	middle
4000293	Kay	44	Childcare worker	49.97	Stopwatches	middle
4008024	Derek	66	Automotive mechanic	31.84	Swimming	old
4008884	Warren	74	Politician	80.99	Tetherball	old
4001050	Barbara	73	Actor	89.56	Gymnastics Mats	old
4003309	Jacob	35	Reporter	55.35	Towed Water Sports	middle
4001561	Arlene	51		184.56	Cheerleading	old
4009270	Christy	71	Loan officer	176.34	Parachutes	old
4002697	Joan	74	Librarian	35.75	Running	old
4008469	William	62	Designer	42.51	Team Handball	old
4006425	Joe	30	Economist	193.11	Sledding	middle
4005513	Valerie	62	Dancer	68.86	Jumping Stilts	old
4004611	Wendy	27	Architect	146.36	Whitewater Rafting	low
4005227	Diane	57	Firefighter	130.52	Medicine Balls	old
4002299	Alison	56	Electrician	197.54	Exercise Balls	old
4002707	Dana	28	Loan officer	5.95	Dominoes	low
4006562	Valerie	44	Computer software engineer	37.29	Lawn Water Slides	middle
Time taken: 0.143 seconds, Fetched: 100 row(s)
hive> 
    > ;
hive> describe out2;
OK
custno              	int                 	                    
firstname           	string              	                    
age                 	int                 	                    
profession          	string              	                    
amount              	double              	                    
product             	string              	                    
level               	string              	                    
Time taken: 0.08 seconds, Fetched: 7 row(s)
hive> crr[9G[Keate table out3(level strib[35G[Kng,amount double)
    > row format delimited
    > fields terminated by '';[30G[K[29G[K,';
OK
Time taken: 0.11 seconds
hive> insert overwrite table out3
    > ae[8G[K[7G[Kselect level,sum(amount) from out2 group by level;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = mohit_20200416123234_596d752d-fcf1-4485-83c1-b9bacee07822
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1587015889950_0005, Tracking URL = http://m2:8088/proxy/application_1587015889950_0005/
Kill Command = /usr/local/hadoop-2.6.5/bin/hadoop job  -kill job_1587015889950_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-04-16 12:35:42,938 Stage-1 map = 0%,  reduce = 0%
2020-04-16 12:35:49,151 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.06 sec
2020-04-16 12:36:50,077 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.06 sec
2020-04-16 12:37:50,880 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.06 sec
2020-04-16 12:38:51,710 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.06 sec
2020-04-16 12:38:59,977 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.38 sec
MapReduce Total cumulative CPU time: 2 seconds 380 msec
Ended Job = job_1587015889950_0005
Loading data to table retail.out3
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.38 sec   HDFS Read: 2794077 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 380 msec
OK
Time taken: 386.721 seconds
hive> s[7G[Kselet[11G[Kct * from out3 limit 20;
OK
low	725221.3399999988
middle	1855861.669999996
old	2529100.310000011
Time taken: 0.319 seconds, Fetched: 3 row(s)
hive> exit;
]0;mohit@edge1: /usr/local/hive[01;32mmohit@edge1[00m:[01;34m/usr/local/hive[00m$ 
]0;mohit@edge1: /usr/local/hive[01;32mmohit@edge1[00m:[01;34m/usr/local/hive[00m$ logout
Connection to edge1 closed.
[01;32mmohit:[01;34m~/MyProjects/BigData/practice/hive[00m (master)[00m $ exit
exit

Script done on 2020-04-16 12:49:54+0530
