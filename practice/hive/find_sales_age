Script started on 2020-04-16 11:13:36+0530
##This file show find Customers based on Age Group
##Hive is running over yarn
mohit:~/MyProjects/BigData/practice/hive (master) $ ssh m1
mohit@m1's password:
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:	   https://landscape.canonical.com
 * Support:	   https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Sat Apr 11 17:44:47 2020 from 192.168.43.10
mohit@m1:~$ cd $HADOOP_INSTALL
mohit@m1:/usr/local/hadoop$ sbin/start-dfs.sh
Starting namenodes on [m1]
m1: starting namenode, logging to /usr/local/hadoop-2.6.5/logs/hadoop-mohit-namenode-m1.out
m3: starting datanode, logging to /usr/local/hadoop-2.6.5/logs/hadoop-mohit-datanode-m3.out
m2: starting datanode, logging to /usr/local/hadoop-2.6.5/logs/hadoop-mohit-datanode-m2.out
m1: starting datanode, logging to /usr/local/hadoop-2.6.5/logs/hadoop-mohit-datanode-m1.out
m3: WARNING: An illegal reflective access operation has occurred
m3: WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar) to method sun.security.krb5.Config.getInstance()
m3: WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
m3: WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
m3: WARNING: All illegal access operations will be denied in a future release
Starting secondary namenodes [m2]
m2: starting secondarynamenode, logging to /usr/local/hadoop-2.6.5/logs/hadoop-mohit-secondarynamenode-m2.out
mohit@m1:/usr/local/hadoop$ logout

Connection to m1 closed.
mohit:~/MyProjects/BigData/practice/hive (master) $ ssh m2
The authenticity of host 'm2 (192.168.43.139)' can't be established.
ECDSA key fingerprint is SHA256:4QS8suCLz8VUYQpDlAqZnASYClsrCqh3etO4PbVt2mc.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'm2,192.168.43.139' (ECDSA) to the list of known hosts.
mohit@m2's password:
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:	   https://landscape.canonical.com
 * Support:	   https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Tue Apr  7 18:03:15 2020 from 192.168.43.10
mohit@m2:~$ cd $HADOOP_INSTALL
mohit@m2:/usr/local/hadoop$ sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop-2.6.5/logs/yarn-mohit-resourcemanager-m2.out
m1: starting nodemanager, logging to /usr/local/hadoop-2.6.5/logs/yarn-mohit-nodemanager-m1.out
m3: starting nodemanager, logging to /usr/local/hadoop-2.6.5/logs/yarn-mohit-nodemanager-m3.out
m2: starting nodemanager, logging to /usr/local/hadoop-2.6.5/logs/yarn-mohit-nodemanager-m2.out
m3: WARNING: An illegal reflective access operation has occurred
m3: WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar) to method sun.security.krb5.Config.getInstance()
m3: WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
m3: WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
m3: WARNING: All illegal access operations will be denied in a future release
mohit@m2:/usr/local/hadoop$ jps
4660 Jps
4101 SecondaryNameNode
4231 ResourceManager
3981 DataNode
4351 NodeManager
mohit@m2:/usr/local/hadoop$ logout
Connection to m2 closed.
mohit:~/MyProjects/BigData/practice/hive (master) $ ssh m3
The authenticity of host 'm3 (192.168.43.61)' can't be established.
ECDSA key fingerprint is SHA256:AB64gicV9wKNfWdc4HSsTNc2e9q64gAHNdEBOftTcxw.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'm3,192.168.43.61' (ECDSA) to the list of known hosts.
mohit@m3's password:
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:	   https://landscape.canonical.com
 * Support:	   https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Fri Apr 10 16:18:59 2020 from 192.168.43.10
mohit@m3:~$ jps
3152 DataNode
3292 NodeManager
3471 Jps
mohit@m3:~$ logout
Connection to m3 closed.
mohit:~/MyProjects/BigData/practice/hive (master) $ ssh m1
mohit@m1's password:
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:	   https://landscape.canonical.com
 * Support:	   https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Thu Apr 16 11:13:44 2020 from 192.168.43.115
mohit@m1:~$ jps
4576 Jps
4160 DataNode
4009 NameNode
4428 NodeManager
mohit@m1:~$ logout
Connection to m1 closed.
mohit:~/MyProjects/BigData/practice/hive (master) $ ssh m2
mohit@m2's password:
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:	   https://landscape.canonical.com
 * Support:	   https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Thu Apr 16 11:14:33 2020 from 192.168.43.115
mohit@m2:~$ cd $HADOOP_INSTALL
mohit@m2:/usr/local/hadoop$ ls
bin  include  libexec	   logs        README.txt  share
etc  lib      LICENSE.txt  NOTICE.txt  sbin
mohit@m2:/usr/local/hadoop$ ls sbin
distribute-exclude.sh	 start-all.cmd	      stop-balancer.sh
hadoop-daemon.sh	 start-all.sh	      stop-dfs.cmd
hadoop-daemons.sh	 start-balancer.sh    stop-dfs.sh
hdfs-config.cmd 	 start-dfs.cmd	      stop-secure-dns.sh
hdfs-config.sh		 start-dfs.sh	      stop-yarn.cmd
httpfs.sh		 start-secure-dns.sh  stop-yarn.sh
kms.sh			 start-yarn.cmd       yarn-daemon.sh
mr-jobhistory-daemon.sh  start-yarn.sh	      yarn-daemons.sh
refresh-namenodes.sh	 stop-all.cmd
slaves.sh		 stop-all.sh
mohit@m2:/usr/local/hadoop$ logout
Connection to m2 closed.
mohit:~/MyProjects/BigData/practice/hive (master) $ ssh edge1
The authenticity of host 'edge1 (192.168.43.10)' can't be established.
ECDSA key fingerprint is SHA256:AB64gicV9wKNfWdc4HSsTNc2e9q64gAHNdEBOftTcxw.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'edge1' (ECDSA) to the list of known hosts.
mohit@edge1's password:
Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-45-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:	   https://landscape.canonical.com
 * Support:	   https://ubuntu.com/advantage

0 packages can be updated.
0 updates are security updates.

Last login: Tue Apr  7 18:04:36 2020 from 192.168.43.139
mohit@edge1:~$ jps
3050 Jps
mohit@edge1:~$ #lets look where the database is kept
mohit@edge1:~$metastore_dbdisilyiningOtherecause metastore_db is lying

mohit@edge1:~$ cd $HIVE_HOME
mohit@edge1:/usr/local/hive$ ls
assign4    examples  LICENSE	     orderitem.java  RELEASE_NOTES.txt
bin	   hcatalog  metastore_db    output	     scripts
conf	   jdbc      NOTICE	     putInHive.sh
derby.log  lib	     orderitem.avsc  README.txt
mohit@edge1:/usr/local/hive$ #yes its in assign4
mohit@edge1:/usr/local/hive$ #lets start with hive
mohit@edge1:/usr/local/hive$ #take a look intotassign4
mohit@edge1:/usr/local/hive$ ls assign4
custs  practicals.txt  txns1.txt
mohit@edge1:/usr/local/hive$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-2.1.0-bin/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/local/apache-hive-2.1.0-bin/lib/hive-common-2.1.0.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> sohow databases;
OK
default
mannadb
retail
Time taken: 1.05 seconds, Fetched: 3 row(s)
hive> --we are using retail
hive> --means comment nin hive., multiline not supported
hive> use retail;
OK
Time taken: 0.027 seconds
hive> shoew tables;
OK
txnrecords
txnrecsbycat
Time taken: 0.101 seconds, Fetched: 2 row(s)
tring,cagetint,professionestring)o string, firstname string, lastname s
    > row orformat delimited
    > fields terminated by '',';
OK
Time taken: 1.124 seconds
hive> show tables;
OK
customer
txnrecords
txnrecsbycat
Time taken: 0.028 seconds, Fetched: 3 row(s)
hive> load data --locate custs in assign4 . we have to put it in dtatabase.
hive> load data load data local inpath 'assign4/custs' into table custommerer;
Loading data to table retail.customer
OK
Time taken: 2.394 seconds
hive> --sedescc customer;
OK
custno			string
firstname		string
lastname		string
age			int
profession		string
Time taken: 0.103 seconds, Fetched: 5 row(s)
hive> select * from customer limit 5;
OK
4000001 Kristina	Chung	55	Pilot
4000002 Paige	Chen	74	Teacher
4000003 Sherri	Melton	34	Firefighter
4000004 Gretchen	Hill	66	Computer hardware engineer
4000005 Karen	Puckett 74	Lawyer
Time taken: 1.28 seconds, Fetched: 5 row(s)
hive> select count(cudstno) froom m customer;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = mohit_20200416112812_c6b5addb-53fe-482c-a5c3-61b39f65a53f
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1587015889950_0001, Tracking URL = http://m2:8088/proxy/application_1587015889950_0001/
Kill Command = /usr/local/hadoop-2.6.5/bin/hadoop job  -kill job_1587015889950_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-04-16 11:28:25,577 Stage-1 map = 0%,  reduce = 0%
2020-04-16 11:29:26,068 Stage-1 map = 0%,  reduce = 0%
2020-04-16 11:30:26,852 Stage-1 map = 0%,  reduce = 0%
2020-04-16 11:31:27,436 Stage-1 map = 0%,  reduce = 0%
2020-04-16 11:31:34,727 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.48 sec
2020-04-16 11:31:43,118 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.3 sec
MapReduce Total cumulative CPU time: 3 seconds 300 msec
Ended Job = job_1587015889950_0001
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.3 sec   HDFS Read: 399557 HDFS Write: 104 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 300 msec
OK
9999
Time taken: 211.636 seconds, Fetched: 1 row(s)
hive> -- it started a map reduce task
hive> -- we coul d watch it on the link provided:
ke-a-dynamic-limit-in-mysqlo(TrackingsURL)s/37466913/how-toarting%20-ma
hive> -- we missed to start job history server
hive> =-- itt can be done
toryserver mohit  @m2:$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start his
hive> --escapigng it now
string,amountedouble,productustring)t,firstname string,age int,profession
    > row format delimited
    > field s terminated by ',';
OK
Time taken: 0.153 seconds
hive> insert desc oiut1;
OK
custno			int
firstname		string
age			int
profession		string
amount			double
product 		string
Time taken: 0.092 seconds, Fetched: 6 row(s)
hive> ---yits same as customer desc customer;
OK
custno			string
firstname		string
lastname		string
age			int
profession		string
Time taken: 0.091 seconds, Fetched: 5 row(s)
hive> -- amount ans d producrt iofnfo  comes from tdesc txnrecords;
OK
txnno			int
txndate 		string
custno			int
amount			double
category		string
product 		string
city			string
state			string
spendby 		string
Time taken: 0.218 seconds, Fetched: 9 row(s)
hive> -- product and amount goes into out1'
hive> insert overwrite table out1
    > select a.custno,a.firstname,a.age,a.professionb.,b.producrt amount,b.product
    > from customer a JOIN txnrecords b ON a.custno=b.custno;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = mohit_20200416114051_8b9cc171-41a7-4aea-a983-1001e6c2c22f
Total jobs = 1
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-2.1.0-bin/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2020-04-16 11:40:56	Starting to launch local task to process map join;	maximum memory = 518979584
2020-04-16 11:40:58	Dump the side-table for tag: 0 with group count: 9999 into file: file:/tmp/mohit/a993596a-a7ff-4821-8655-f3ea9e01cab4/hive_2020-04-16_11-40-51_478_8153478797335008366-1/-local-10002/HashTable-Stage-4/MapJoin-mapfile00--.hashtable
2020-04-16 11:40:58	Uploaded 1 File to: file:/tmp/mohit/a993596a-a7ff-4821-8655-f3ea9e01cab4/hive_2020-04-16_11-40-51_478_8153478797335008366-1/-local-10002/HashTable-Stage-4/MapJoin-mapfile00--.hashtable (553952 bytes)
2020-04-16 11:40:58	End of local task; Time Taken: 1.519 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1587015889950_0002, Tracking URL = http://m2:8088/proxy/application_1587015889950_0002/
Kill Command = /usr/local/hadoop-2.6.5/bin/hadoop job  -kill job_1587015889950_0002
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 0
2020-04-16 11:41:07,090 Stage-4 map = 0%,  reduce = 0%
2020-04-16 11:41:14,328 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 2.89 sec
MapReduce Total cumulative CPU time: 2 seconds 890 msec
Ended Job = job_1587015889950_0002
Loading data to table retail.out1
MapReduce Jobs Launched:
Stage-Stage-4: Map: 1	Cumulative CPU: 2.89 sec   HDFS Read: 4426457 HDFS Write: 2530247 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 890 msec
OK
Time taken: 25.285 seconds
hive> select * from out1 limit 100;
OK
4007024 Cameron 59	Actor	40.33	Cardio Machine Accessories
4006742 Gregory 36	Accountant	198.44	Weightlifting Gloves
4009775 Ruby	44	Designer	5.58	Weightlifting Machine Accessories
4002199 Keith	44	Police officer	198.19	Gymnastics Rings
4002613 Hugh	43	Engineering technician	98.81	Field Hockey
4007591 Jennifer	54	Electrician	193.63	Camping & Backpacking & Hiking
4002190 Sheryl	62	Designer	27.89	Jigsaw Puzzles
4002964 Ken	67	Recreation and fitness worker	96.01	Sandboxes
4007361 Terri	52	Loan officer	10.44	Snowmobiling
4004798 Geoffrey	65	Chemist 152.46	Bungee Jumping
4004646 Frank	64	Computer software engineer	180.28	Archery
4008071 Ruby	46	Agricultural and food scientist 121.39	Swing Sets
4002473 Regina	68	Pilot	41.52	Bowling
4003268 Lillian 35	Firefighter	107.8	Field Hockey
4004613 Dennis	62	Judge	36.81	Vaulting Horses
4003179 Kelly	71	Engineering technician	137.64	Fencing
4009135 Elaine	42	Electrician	35.56	Free Weight Bars
4006679 Jeanne	22	Judge	75.55	Scuba Diving & Snorkeling
4002444 Jeanne	72	Therapist	88.65	Baseball
4008871 Nina	69	Politician	51.81	Life Jackets
4001364 Nancy	75	Judge	41.55	Weightlifting Belts
4003144 Samantha	65	Veterinarian	45.79	Parachutes
4006131 Ken	25	Lawyer	19.64	Kitesurfing
4007596 Sandra	30	Actor	99.5	Gymnastics Rings
4009341 Todd	31	Farmer	151.2	Surfing
4003760 Suzanne 25	Computer support specialist	144.2	Darts
4000403 Leo	65	Pilot	31.58	Wrestling
4005211 Jay	36	Photographer	66.4	Mahjong
4001864 Alexandra	29	Carpenter	79.78	Cricket
4005691 Brenda	54	Reporter	126.9	Hunting
4009693 Paul	43	Veterinarian	47.05	Swimming
4002130 Colleen 55	Civil engineer	5.03	Dice & Dice Sets
4007790 Clifford	24	Librarian	20.13	Soccer
4005337 Allison 73	Human resources assistant	154.15	Lawn Games
4000663 Claire	29	Police officer	98.96	Indoor Volleyball
4006967 Martin	54	Computer support specialist	185.26	Board Games
4009055 Eva	68	Veterinarian	35.66	Football
4005737 Dana	72	Artist	20.2	Shooting Games
4000175 Ben	35	Computer support specialist	150.6	Camping & Backpacking & Hiking
4001873 Phyllis 24	Farmer	174.36	Swing Sets
4006442 Jerome	29	Automotive mechanic	165.1	Cheerleading
4004237 Gregory 74	Actor	28.11	Bowling
4007470 Rick	56	Coach	38.52	Tetherball
4002554 Laura	61	Financial analyst	32.34	Water Polo
4001041 Crystal 39	Psychologist	135.37	Surfing
4005646 Tammy	65	Reporter	90.04	Abdominal Equipment
4005580 Eddie	72	Automotive mechanic	52.29	Vaulting Horses
4009698 Hilda	48	Firefighter	100.1	Swing Sets
4009252 Phillip 23	Firefighter	157.94	Exercise Bands
4003896 Nicole	74	Judge	144.59	Jumping Stilts
4005578 Hilda	64	Architect	55.93	Pogo Sticks
4002323 Roger	29	Recreation and fitness worker	32.65	Life Jackets
4008289 David	25	Electrical engineer	44.82	Lawn Water Slides
4003091 Joanne	59	Politician	44.46	Scuba Diving & Snorkeling
4007357 Clara	61	Computer software engineer	154.87	Running
4004961 Katharine	34	Computer hardware engineer	106.11	Swimming
4008744 Larry	47	Police officer	176.63	Geocaching
4007745 Sally	66	Dancer	178.2	Skating
4003248 Randall 33	Computer support specialist	194.86	Windsurfing
4002854 Leon	51	Pharmacist	21.43	Snowboarding
4004874 Danielle	48	Reporter	118.18	Cardio Machine Accessories
4009680 Ted	38	Electrician	41.14	Weightlifting Machine Accessories
4000539 Hugh	71	Firefighter	100.93	Beach Volleyball
4009719 Greg	71	Musician	129.26	Downhill Skiing
4008455 Laura	59	Computer hardware engineer	105.24	Weightlifting Machine Accessories
4005887 Neil	45	Physicist	66.06	Riding Scooters
4006293 Nicholas	63	Teacher 89.14	Tetherball
4002526 Bob	40		159.14	Poker Chips & Sets
4006736 Tamara	23	Loan officer	171.57	Ballet Bars
4005403 Ruby	48	Veterinarian	89.91	Softball
4006291 Ruby	57	Police officer	32.28	Skating
4009360 Raymond 39	Doctor	152.21	Cricket
4001305 Andrea	56	Psychologist	150.82	Skateboarding
4008653 Toni	39	Athlete 133.2	Riding Scooters
4002106 Lester	50	Coach	148.31	Portable Electronic Games
4006262 Denise	33	Financial analyst	125.28	Wrestling
4003860 Pamela	71	Veterinarian	170.05	Trampolines
4000458 Harold	40	Musician	74.06	Jigsaw Puzzles
4002200 Sandy	33	Financial analyst	175.24	Baseball
4005751 Paige	29	Nurse	39.8	Springboards
4005625 Herman	30	Architect	174.82	Ice Climbing
4003500 Dana	51	Artist	188.9	Swimming
4001098 Curtis	50	Loan officer	21.23	Gymnastics Rings
4007259 Aaron	38	Engineering technician	22.42	Rock Climbing
4000293 Kay	44	Childcare worker	49.97	Stopwatches
4008024 Derek	66	Automotive mechanic	31.84	Swimming
4008884 Warren	74	Politician	80.99	Tetherball
4001050 Barbara 73	Actor	89.56	Gymnastics Mats
4003309 Jacob	35	Reporter	55.35	Towed Water Sports
4001561 Arlene	51		184.56	Cheerleading
4009270 Christy 71	Loan officer	176.34	Parachutes
4002697 Joan	74	Librarian	35.75	Running
4008469 William 62	Designer	42.51	Team Handball
4006425 Joe	30	Economist	193.11	Sledding
4005513 Valerie 62	Dancer	68.86	Jumping Stilts
4004611 Wendy	27	Architect	146.36	Whitewater Rafting
4005227 Diane	57	Firefighter	130.52	Medicine Balls
4002299 Alison	56	Electrician	197.54	Exercise Balls
4002707 Dana	28	Loan officer	5.95	Dominoes
4006562 Valerie 44	Computer software engineer	37.29	Lawn Water Slides
Time taken: 0.211 seconds, Fetched: 100 row(s)
string,amounttdouble,productnstring,ilevelmstring)g,age int,profession
    > row format delimited
    > fields terminated by ',';
OK
Time taken: 0.217 seconds
hive> insert overrwrite tadesc out2;
OK
custno			int
firstname		string
age			int
profession		string
amount			double
product 		string
level			string
Time taken: 0.059 seconds, Fetched: 7 row(s)
hive> desc out1;
OK
custno			int
firstname		string
age			int
profession		string
amount			double
product 		string
Time taken: 0.06 seconds, Fetched: 6 row(s)
hive> -- out2 has extra rcolumn level we put a mapping of age there
hive> insert overwriete table out2
    > select *,case
    > when age<30 then 'low'
    > ehwhen age>=30 and age <50  50 then 'middle';
    > when age>=50 then 'old'
    > else 'others'
    > end
    > from out1;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = mohit_20200416115035_36bbb2bb-03fd-4c91-a232-50138eb2e462
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1587015889950_0003, Tracking URL = http://m2:8088/proxy/application_1587015889950_0003/
Kill Command = /usr/local/hadoop-2.6.5/bin/hadoop job  -kill job_1587015889950_0003
Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 0
2020-04-16 11:56:37,743 Stage-1 map = 0%,  reduce = 0%
Ended Job = job_1587015889950_0003 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1:	HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
hive> ---------connection exception occureerred - connect exception occurred lets trretry
hive> -- connect exception occurred lets retryfrom out1;endlse 'others'when age>=50 then 'old'30 and age < 50 then 'middle'<30 then 'low'select *,caseinsert overwrite table out2-- out2 has extra column level we put a mapping of age thereinsert overwrite table out2
    > insert overwrite table out2-- connect exception occurred lets retryfrom out1;endlse 'others'when age>=50 then 'old'30 and age < 50 then 'middle'<30 then 'low'select *,caseinsert overwrite table out2select *,case
    > select *,caseinsert overwrite table out2-- connect exception occurred lets retryfrom out1;endlse 'others'when age>=50 then 'old'30 and age < 50 then 'middle'50 then 'old'30 and age < 50 then 'middle'<30 then 'low'select *,casewhen age<30 then 'low'
    > when age<30 then 'low'select *,caseinsert overwrite table out2-- connect exception occurred lets retryfrom out1;endlse 'others'when age>=50 then 'old'30 and age < 50 then 'middle'<30 then 'low'select *,casewhen age<30 then 'low'>=30 and age < 50 then 'middle'
    > when age>=30 and age < 50 then 'middle'<30 then 'low'select *,caseinsert overwrite table out2-- connect exception occurred lets retryfrom out1;endlse 'others'when age>=50 then 'old'30 and age < 50 then 'middle'50 then 'old'
    > when age>=50 then 'old'30 and age < 50 then 'middle'<30 then 'low'select *,caseinsert overwrite table out2-- connect exception occurred lets retryfrom out1;endlse 'others'when age>=50 then 'old'else 'others'
    > else 'others'when age>=50 then 'old'30 and age < 50 then 'middle'<30 then 'low'select *,caseinsert overwrite table out2-- connect exception occurred lets retryfrom out1;-- connect exception occurred lets retryinsert overwrite table out2select *,casewhen age<30 then 'low'>=30 and age < 50 then 'middle'50 then 'old'else 'others'else 'others'when age>=50 then 'old'30 and age < 50 then 'middle'<30 then 'low'select *,caseinsert overwrite table out2-- connect exception occurred lets retryfrom out1;endlse 'others'nd
    > endlse 'others'when age>=50 then 'old'30 and age < 50 then 'middle'<30 then 'low'select *,caseinsert overwrite table out2-- connect exception occurred lets retryfrom out1;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = mohit_20200416120649_a2fcb52b-ecf3-4ca4-aa3e-b91194a16f21
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1587015889950_0004, Tracking URL = http://m2:8088/proxy/application_1587015889950_0004/
Kill Command = /usr/local/hadoop-2.6.5/bin/hadoop job  -kill job_1587015889950_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2020-04-16 12:06:56,176 Stage-1 map = 0%,  reduce = 0%
2020-04-16 12:07:56,990 Stage-1 map = 0%,  reduce = 0%
2020-04-16 12:08:57,532 Stage-1 map = 0%,  reduce = 0%
2020-04-16 12:09:58,050 Stage-1 map = 0%,  reduce = 0%
2020-04-16 12:10:07,482 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.17 sec
MapReduce Total cumulative CPU time: 2 seconds 170 msec
Ended Job = job_1587015889950_0004
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://m1:9000/user/hive/warehouse/retail.db/out2/.hive-staging_hive_2020-04-16_12-06-49_558_5063276530985635205-1/-ext-10000
Loading data to table retail.out2
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1	Cumulative CPU: 2.17 sec   HDFS Read: 2535512 HDFS Write: 2784776 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 170 msec
OK
Time taken: 199.287 seconds
hive> descrivbe elect select * from out2 limit 1000;
OK
4007024 Cameron 59	Actor	40.33	Cardio Machine Accessories	old
4006742 Gregory 36	Accountant	198.44	Weightlifting Gloves	middle
4009775 Ruby	44	Designer	5.58	Weightlifting Machine Accessories	middle
4002199 Keith	44	Police officer	198.19	Gymnastics Rings	middle
4002613 Hugh	43	Engineering technician	98.81	Field Hockey	middle
4007591 Jennifer	54	Electrician	193.63	Camping & Backpacking & Hiking	old
4002190 Sheryl	62	Designer	27.89	Jigsaw Puzzles	old
4002964 Ken	67	Recreation and fitness worker	96.01	Sandboxes	old
4007361 Terri	52	Loan officer	10.44	Snowmobiling	old
4004798 Geoffrey	65	Chemist 152.46	Bungee Jumping	old
4004646 Frank	64	Computer software engineer	180.28	Archery old
4008071 Ruby	46	Agricultural and food scientist 121.39	Swing Sets	middle
4002473 Regina	68	Pilot	41.52	Bowling old
4003268 Lillian 35	Firefighter	107.8	Field Hockey	middle
4004613 Dennis	62	Judge	36.81	Vaulting Horses old
4003179 Kelly	71	Engineering technician	137.64	Fencing old
4009135 Elaine	42	Electrician	35.56	Free Weight Bars	middle
4006679 Jeanne	22	Judge	75.55	Scuba Diving & Snorkeling	low
4002444 Jeanne	72	Therapist	88.65	Baseball	old
4008871 Nina	69	Politician	51.81	Life Jackets	old
4001364 Nancy	75	Judge	41.55	Weightlifting Belts	old
4003144 Samantha	65	Veterinarian	45.79	Parachutes	old
4006131 Ken	25	Lawyer	19.64	Kitesurfing	low
4007596 Sandra	30	Actor	99.5	Gymnastics Rings	middle
4009341 Todd	31	Farmer	151.2	Surfing middle
4003760 Suzanne 25	Computer support specialist	144.2	Darts	low
4000403 Leo	65	Pilot	31.58	Wrestling	old
4005211 Jay	36	Photographer	66.4	Mahjong middle
4001864 Alexandra	29	Carpenter	79.78	Cricket low
4005691 Brenda	54	Reporter	126.9	Hunting old
4009693 Paul	43	Veterinarian	47.05	Swimming	middle
4002130 Colleen 55	Civil engineer	5.03	Dice & Dice Sets	old
4007790 Clifford	24	Librarian	20.13	Soccer	low
4005337 Allison 73	Human resources assistant	154.15	Lawn Games	old
4000663 Claire	29	Police officer	98.96	Indoor Volleyball	low
4006967 Martin	54	Computer support specialist	185.26	Board Games	old
4009055 Eva	68	Veterinarian	35.66	Football	old
4005737 Dana	72	Artist	20.2	Shooting Games	old
4000175 Ben	35	Computer support specialist	150.6	Camping & Backpacking & Hiking	middle
4001873 Phyllis 24	Farmer	174.36	Swing Sets	low
4006442 Jerome	29	Automotive mechanic	165.1	Cheerleading	low
4004237 Gregory 74	Actor	28.11	Bowling old
4007470 Rick	56	Coach	38.52	Tetherball	old
4002554 Laura	61	Financial analyst	32.34	Water Polo	old
4001041 Crystal 39	Psychologist	135.37	Surfing middle
4005646 Tammy	65	Reporter	90.04	Abdominal Equipment	old
4005580 Eddie	72	Automotive mechanic	52.29	Vaulting Horses old
4009698 Hilda	48	Firefighter	100.1	Swing Sets	middle
4009252 Phillip 23	Firefighter	157.94	Exercise Bands	low
4003896 Nicole	74	Judge	144.59	Jumping Stilts	old
4005578 Hilda	64	Architect	55.93	Pogo Sticks	old
4002323 Roger	29	Recreation and fitness worker	32.65	Life Jackets	low
4008289 David	25	Electrical engineer	44.82	Lawn Water Slides	low
4003091 Joanne	59	Politician	44.46	Scuba Diving & Snorkeling	old
4007357 Clara	61	Computer software engineer	154.87	Running old
4004961 Katharine	34	Computer hardware engineer	106.11	Swimming	middle
4008744 Larry	47	Police officer	176.63	Geocaching	middle
4007745 Sally	66	Dancer	178.2	Skating old
4003248 Randall 33	Computer support specialist	194.86	Windsurfing	middle
4002854 Leon	51	Pharmacist	21.43	Snowboarding	old
4004874 Danielle	48	Reporter	118.18	Cardio Machine Accessories	middle
4009680 Ted	38	Electrician	41.14	Weightlifting Machine Accessories	middle
4000539 Hugh	71	Firefighter	100.93	Beach Volleyball	old
4009719 Greg	71	Musician	129.26	Downhill Skiing old
4008455 Laura	59	Computer hardware engineer	105.24	Weightlifting Machine Accessories	old
4005887 Neil	45	Physicist	66.06	Riding Scooters middle
4006293 Nicholas	63	Teacher 89.14	Tetherball	old
4002526 Bob	40		159.14	Poker Chips & Sets	middle
4006736 Tamara	23	Loan officer	171.57	Ballet Bars	low
4005403 Ruby	48	Veterinarian	89.91	Softball	middle
4006291 Ruby	57	Police officer	32.28	Skating old
4009360 Raymond 39	Doctor	152.21	Cricket middle
4001305 Andrea	56	Psychologist	150.82	Skateboarding	old
4008653 Toni	39	Athlete 133.2	Riding Scooters middle
4002106 Lester	50	Coach	148.31	Portable Electronic Games	old
4006262 Denise	33	Financial analyst	125.28	Wrestling	middle
4003860 Pamela	71	Veterinarian	170.05	Trampolines	old
4000458 Harold	40	Musician	74.06	Jigsaw Puzzles	middle
4002200 Sandy	33	Financial analyst	175.24	Baseball	middle
4005751 Paige	29	Nurse	39.8	Springboards	low
4005625 Herman	30	Architect	174.82	Ice Climbing	middle
4003500 Dana	51	Artist	188.9	Swimming	old
4001098 Curtis	50	Loan officer	21.23	Gymnastics Rings	old
4007259 Aaron	38	Engineering technician	22.42	Rock Climbing	middle
4000293 Kay	44	Childcare worker	49.97	Stopwatches	middle
4008024 Derek	66	Automotive mechanic	31.84	Swimming	old
4008884 Warren	74	Politician	80.99	Tetherball	old
4001050 Barbara 73	Actor	89.56	Gymnastics Mats old
4003309 Jacob	35	Reporter	55.35	Towed Water Sports	middle
4001561 Arlene	51		184.56	Cheerleading	old
4009270 Christy 71	Loan officer	176.34	Parachutes	old
4002697 Joan	74	Librarian	35.75	Running old
4008469 William 62	Designer	42.51	Team Handball	old
4006425 Joe	30	Economist	193.11	Sledding	middle
4005513 Valerie 62	Dancer	68.86	Jumping Stilts	old
4004611 Wendy	27	Architect	146.36	Whitewater Rafting	low
4005227 Diane	57	Firefighter	130.52	Medicine Balls	old
4002299 Alison	56	Electrician	197.54	Exercise Balls	old
4002707 Dana	28	Loan officer	5.95	Dominoes	low
4006562 Valerie 44	Computer software engineer	37.29	Lawn Water Slides	middle
Time taken: 0.143 seconds, Fetched: 100 row(s)
hive>
    > ;
hive> describe out2;
OK
custno			int
firstname		string
age			int
profession		string
amount			double
product 		string
level			string
Time taken: 0.08 seconds, Fetched: 7 row(s)
hive> crreate table out3(level stribng,amount double)
    > row format delimited
    > fields terminated by '';,';
OK
Time taken: 0.11 seconds
hive> insert overwrite table out3
    > aeselect level,sum(amount) from out2 group by level;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = mohit_20200416123234_596d752d-fcf1-4485-83c1-b9bacee07822
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1587015889950_0005, Tracking URL = http://m2:8088/proxy/application_1587015889950_0005/
Kill Command = /usr/local/hadoop-2.6.5/bin/hadoop job  -kill job_1587015889950_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2020-04-16 12:35:42,938 Stage-1 map = 0%,  reduce = 0%
2020-04-16 12:35:49,151 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.06 sec
2020-04-16 12:36:50,077 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.06 sec
2020-04-16 12:37:50,880 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.06 sec
2020-04-16 12:38:51,710 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.06 sec
2020-04-16 12:38:59,977 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.38 sec
MapReduce Total cumulative CPU time: 2 seconds 380 msec
Ended Job = job_1587015889950_0005
Loading data to table retail.out3
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.38 sec   HDFS Read: 2794077 HDFS Write: 136 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 380 msec
OK
Time taken: 386.721 seconds
hive> sseletct * from out3 limit 20;
OK
low	725221.3399999988
middle	1855861.669999996
old	2529100.310000011
Time taken: 0.319 seconds, Fetched: 3 row(s)
hive> exit;
mohit@edge1:/usr/local/hive$
mohit@edge1:/usr/local/hive$ logout
Connection to edge1 closed.
mohit:~/MyProjects/BigData/practice/hive (master) $ exit
exit

Script done on 2020-04-16 12:49:54+0530
